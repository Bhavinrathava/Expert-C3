{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import nltk\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgLhb956fokH",
        "outputId": "f4fa16f9-c0a0-4746-c4a2-09485f5e73c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def download_squad(version=1.1):\n",
        "    assert version in [1.1, 2.0], \"Version must be either 1.1 or 2.0\"\n",
        "    base_url = f\"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "    train_file = f\"train-v{version}.json\"\n",
        "    dev_file = f\"dev-v{version}.json\"\n",
        "\n",
        "    for file in [train_file, dev_file]:\n",
        "        url = base_url + file\n",
        "        response = requests.get(url)\n",
        "        with open(file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {file}\")\n",
        "\n",
        "def load_squad(version=2.0):\n",
        "    train_file = f\"train-v{version}.json\"\n",
        "    dev_file = f\"dev-v{version}.json\"\n",
        "\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        train_data = json.load(f)\n",
        "\n",
        "    with open(dev_file, 'r', encoding='utf-8') as f:\n",
        "        dev_data = json.load(f)\n",
        "\n",
        "    return {'train': train_data['data'], 'validation': dev_data['data']}"
      ],
      "metadata": {
        "id": "tyivqH88g4F_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_squad(2.0)\n",
        "squad = load_squad(2.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL3NHpgGg4v9",
        "outputId": "14cf3fee-6171-4d40-e5ad-717557194fc1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded train-v2.0.json\n",
            "Downloaded dev-v2.0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLuIsByhcimJ",
        "outputId": "9b0beb1f-1add-4ef9-8741-0da9c18d4544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess_squad(dset):\n",
        "    examples = []\n",
        "\n",
        "    for article in tqdm(dset):\n",
        "        for paragraph in article['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answers = qa.get('answers', [])\n",
        "\n",
        "                if len(answers) == 0 or answers[0]['text'] == \"\":\n",
        "                    # Handle unanswerable questions\n",
        "                    # You can decide how to handle these. Here's an example:\n",
        "                    examples.append({'question': question, 'sentence': context, 'label': 0})\n",
        "                    continue\n",
        "\n",
        "                # Processing answerable questions\n",
        "                answer_start = answers[0]['answer_start']\n",
        "                answer_text = answers[0]['text']\n",
        "                answer_end = answer_start + len(answer_text)\n",
        "\n",
        "                pos_sent = None\n",
        "                sent_start = 0\n",
        "                sentences = nltk.sent_tokenize(context)\n",
        "                for sentence in sentences:\n",
        "                    sent_end = sent_start + len(sentence)\n",
        "                    if sent_start <= answer_start and sent_end >= answer_end:\n",
        "                        pos_sent = sentence\n",
        "                        break\n",
        "                    sent_start = sent_end\n",
        "\n",
        "                # Create positive and negative examples\n",
        "                if pos_sent:\n",
        "                    examples.append({'question': question, 'sentence': pos_sent, 'label': 1})\n",
        "\n",
        "                    neg_sentences = [s for s in sentences if s != pos_sent]\n",
        "                    if neg_sentences:\n",
        "                        neg_sent = random.choice(neg_sentences)\n",
        "                        examples.append({'question': question, 'sentence': neg_sent, 'label': 0})\n",
        "\n",
        "    return examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_examples  =   preprocess_squad(squad['train'])\n",
        "train_dloader   =   DataLoader(train_examples, shuffle=True,batch_size=8)\n",
        "\n",
        "val_examples    =   preprocess_squad(squad['validation'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSawHarPcpK-",
        "outputId": "9b917c6d-eeca-4de8-81b8-36164bd5ea92"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 442/442 [00:15<00:00, 29.35it/s]\n",
            "100%|██████████| 35/35 [00:00<00:00, 35.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "cross_model = CrossEncoder('distilroberta-base', num_labels=1)"
      ],
      "metadata": {
        "id": "APoWpxjUeDKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
        "\n",
        "evaluator   =   CEBinaryClassificationEvaluator.from_input_examples(val_examples)"
      ],
      "metadata": {
        "id": "xBjVxuvTeEbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"Contextual_Compressor\")\n",
        "\n",
        "def callback_model(score, epoch, steps):\n",
        "    wandb.log({\"train/epoch\" : epoch,\n",
        "                \"train/steps\": steps,\n",
        "                \"train/score\" : score})"
      ],
      "metadata": {
        "id": "G443H423eHbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs  =   4\n",
        "warmup_prop =   0.1\n",
        "eval_steps  =  1000\n",
        "\n",
        "cross_model.fit(train_dataloader=train_dloader,\n",
        "        evaluator=evaluator,\n",
        "        epochs=num_epochs,\n",
        "        warmup_steps=warmup_prop*num_epochs*len(train_dloader),\n",
        "        evaluation_steps=eval_steps,\n",
        "        callback=callback_model,\n",
        "        show_progress_bar=True)"
      ],
      "metadata": {
        "id": "zW0SzZhOeI2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context, question   =   squad['validation'][4]['context'], squad['validation'][4]['question']\n",
        "\n",
        "sentences   =   nltk.sent_tokenize(context)\n",
        "\n",
        "print(question)\n",
        "print(\"---- \\n\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    print(cross_model.predict([sentence, question]))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "1AjdRP6beXL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}